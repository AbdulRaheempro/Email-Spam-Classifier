# -*- coding: utf-8 -*-
"""spamdetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13m5fFhcJNsA0UTnV2Yf6XTKwMnsQR6fy
"""

import numpy as np
import pandas as pd

s=pd.read_csv("/content/spam.csv",encoding='latin-1')
a=s

a.sample(3)

a.info()

a.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

a.rename(columns={'v1':"target",'v2':'text'},inplace=True)

a.head(2)

from sklearn.preprocessing import LabelEncoder

encoder=LabelEncoder()

a['target']=encoder.fit_transform(a['target'])

a.head()

a.isnull().sum()

a.duplicated().sum()

a.drop_duplicates(inplace=True)

a.duplicated().sum()

a.shape

a['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(a['target'].value_counts(),labels=['ham','spam'],autopct="%0.2f'")
plt.show()

!pip install nltk

import nltk
nltk.download('punkt')

nltk.download('punkt_tab')

a['num_characters']=a['text'].apply(len)

a.head(2)

a['num_words']=a['text'].apply(lambda x:len(nltk.word_tokenize(x)))

a.head(3)

a['sentences']=a['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

a.head(2)

a.describe()

a[a['target']==0].describe()

a[a['target']==1].describe()

import seaborn as sns

sns.histplot(a[a['target']==0]['num_characters'])
sns.histplot(a[a['target']==1]['num_characters'],color='red')

sns.pairplot(a,hue='target')

from nltk.corpus import stopwords
import string

nltk.download('stopwords')

def texttransform(text):
  text=text.lower()
  text=nltk.word_tokenize(text)
  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)
  text=y[:]
  y.clear()
  for i in text:
    if i not in stopwords.words("english") and i not in string.punctuation:
      y.append(i)
  text=y[:]
  y.clear()
  for i in text:
    y.append(ps.stem(i))
  return " ".join(y)

texttransform("hi how ARE raheem %%")

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

a['transform_text']=a['text'].apply(texttransform)

a.head(4)

from wordcloud import WordCloud

wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc=wc.generate(a[a['target']==1]['transform_text'].str.cat(sep=' '))

plt.imshow(spam_wc)

ham_wc=wc.generate(a[a['target']==0]['transform_text'].str.cat(sep=' '))

plt.imshow(spam_wc)

a.head(3)

from collections import Counter

spam=a[a['target']==1]['transform_text'].str.cat(sep=' ')

words=spam.split()

word_count=Counter(words)

topspam=word_count.most_common(30)

topspam

spam=a[a['target']==0]['transform_text'].str.cat(sep=' ')
words=spam.split()
word_count=Counter(words)
topham=word_count.most_common(30)



topham

from sklearn.feature_extraction.text import CountVectorizer

cv=CountVectorizer()

tf=TfidfVectorizer(max_features=3000)

X=tf.fit_transform(a['transform_text']).toarray()

# from sklearn.preprocessing import MinMaxScaler
# scaler=MinMaxScaler()
# X=scaler.fit_transform(X)

y=a['target'].values

y

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(
    X,y,test_size=0.2,random_state=2

)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb=GaussianNB()
mb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(X_train,y_train)

y_pred=gnb.predict(X_test)

accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

mb.fit(X_train,y_train)
y_pred=mb.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(precision_score(y_test,y_pred))

bnb.fit(X_train,y_train)
y_pred=bnb.predict(X_test)
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(precision_score(y_test,y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score

clfs = {
    "SVC": SVC(kernel='sigmoid', gamma=1.0),
    "KNN": KNeighborsClassifier(),
    "MultinomialNB": MultinomialNB(),
    "DecisionTree": DecisionTreeClassifier(max_depth=5),
    "LogisticRegression": LogisticRegression(solver='liblinear', penalty='l1'),
    "RandomForest": RandomForestClassifier(n_estimators=50, random_state=2),
    "AdaBoost": AdaBoostClassifier(n_estimators=50, random_state=2),
    "Bagging": BaggingClassifier(n_estimators=50, random_state=2),
    "ExtraTrees": ExtraTreesClassifier(n_estimators=50, random_state=2),
    "GBDT": GradientBoostingClassifier(n_estimators=50, random_state=2),
    "XGB": XGBClassifier(n_estimators=50, random_state=2, use_label_encoder=False, eval_metric='logloss')
}

def train_classifier(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    return acc, prec

acc_scores = []
prec_scores = []

for name, clf in clfs.items():
    current_accuracy, current_precision = train_classifier(
        clf, X_train, y_train, X_test, y_test
    )
    print("For", name)
    print("Accuracy:", current_accuracy)
    print("Precision:", current_precision)
    acc_scores.append(current_accuracy)
    prec_scores.append(current_precision)







for name, clf in clfs.items():
    current_accuracy, current_precision = train_classifier(
        clf, X_train, y_train, X_test, y_test
    )
    print("For", name)
    print("Accuracy:", current_accuracy)
    print("Precision:", current_precision)
    acc_scores.append(current_accuracy)
    prec_scores.append(current_precision)

import pickle
pickle.dump(tf,open("vectorizer.pkl",'wb'))
pickle.dump(mb,open("model.pkl",'wb'))